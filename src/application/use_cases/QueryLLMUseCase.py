# src/application/use_cases/QueryLLMUseCase.py
from typing import AsyncGenerator, Union
import json

from src.core.entities.QueryEntitiesTODO import QueryRequest, LLMResponse, LLMStreamResponse
from src.core.entities.UserEntities import ListUserPsychStatus
from src.core.interfaces import ILLMProvider, IChatStorage
from src.infrastructure.burnout_analysis_parser.BurnoutAnalysisService import BurnoutAnalysisService
from src.infrastructure.llm.DeepSeekLLM import DeepSeekLLM
from src.infrastructure.mongodb_store.MongoDBChatStorage import MongoDBChatStorage


class QueryLLMUseCase:
    def __init__(self, llm_provider: ILLMProvider, chat_storage: IChatStorage):
        self.llm_provider = llm_provider
        self.chat_storage = chat_storage
        self.analysis_service = BurnoutAnalysisService()

        # ÐŸÑ€Ð¾Ð¼Ð¿Ñ‚ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²
        self.analysis_prompt = """
            Ð¢Ð« Ð”ÐžÐ›Ð–Ð•Ð Ð’Ð«Ð’Ð•Ð¡Ð¢Ð˜ Ð¢ÐžÐ›Ð¬ÐšÐž JSON Ð‘Ð•Ð— Ð›Ð®Ð‘Ð«Ð¥ Ð”ÐžÐŸÐžÐ›ÐÐ˜Ð¢Ð•Ð›Ð¬ÐÐ«Ð¥ Ð¡Ð›ÐžÐ’, ÐšÐžÐœÐœÐ•ÐÐ¢ÐÐ Ð˜Ð•Ð’ Ð˜Ð›Ð˜ Ð¤ÐžÐ ÐœÐÐ¢Ð˜Ð ÐžÐ’ÐÐÐ˜Ð¯.
            Ð¢Ñ‹ Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿ÑÐ¸Ñ…Ð¾Ð»Ð¾Ð³. ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹ Ð½Ð° Ð¾Ð¿Ñ€Ð¾Ñ Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð²Ñ‹Ð³Ð¾Ñ€Ð°Ð½Ð¸Ñ (MBI) Ð¸ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹.

            Ð¨ÐºÐ°Ð»Ñ‹ Ð¾Ñ†ÐµÐ½ÐºÐ¸:
            - Ð­Ð¼Ð¾Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð¸ÑÑ‚Ð¾Ñ‰ÐµÐ½Ð¸Ðµ (0-54): 0-15 Ð½Ð¸Ð·ÐºÐ¸Ð¹, 16-24 ÑÑ€ÐµÐ´Ð½Ð¸Ð¹, 25+ Ð²Ñ‹ÑÐ¾ÐºÐ¸Ð¹
            - Ð”ÐµÐ¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ (0-30): 0-5 Ð½Ð¸Ð·ÐºÐ¸Ð¹, 6-10 ÑÑ€ÐµÐ´Ð½Ð¸Ð¹, 11+ Ð²Ñ‹ÑÐ¾ÐºÐ¸Ð¹
            - Ð ÐµÐ´ÑƒÐºÑ†Ð¸Ñ Ð¿Ñ€Ð¾Ñ„. Ð´Ð¾ÑÑ‚Ð¸Ð¶ÐµÐ½Ð¸Ð¹ (0-48): 0-30 Ð½Ð¸Ð·ÐºÐ¸Ð¹, 31-36 ÑÑ€ÐµÐ´Ð½Ð¸Ð¹, 37+ Ð²Ñ‹ÑÐ¾ÐºÐ¸Ð¹
            - Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¹ Ð¸Ð½Ð´ÐµÐºÑ = (Ð­Ð˜ + Ð”ÐŸ + Ð ÐŸÐ”) / 132

            Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚ Ñ‚Ð²Ð¾ÐµÐ³Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚Ð° (Ð’ Ð¡Ð¢Ð ÐžÐ“ÐžÐœ JSON-Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ):
            {
              "emotional_exhaustion": 0-54,
              "depersonalization": 0-30, 
              "reduction_of_achievements": 0-48,
              "burnout_index": 0.0-1.0,
              "recommendations": ["Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ñ emotional_exhaustion", "Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ñ depersonalization", "Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ñ reduction_of_achievements", "ÐžÐ±Ñ‰Ð°Ñ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ñ"]
            }
            ÐÐ• ÐŸÐ˜Ð¨Ð˜ ÐÐ˜ÐšÐÐšÐ˜Ð¥ ÐŸÐ Ð•Ð”Ð˜Ð¡Ð›ÐžÐ’Ð˜Ð™, ÐšÐžÐœÐœÐ•ÐÐ¢ÐÐ Ð˜Ð•Ð’, Ð’ÐžÐŸÐ ÐžÐ¡ÐžÐ’ Ð˜Ð›Ð˜ Ð—ÐÐšÐ›Ð®Ð§Ð•ÐÐ˜Ð™.
            ÐÐ• Ð˜Ð¡ÐŸÐžÐ›Ð¬Ð—Ð£Ð™ MARKDOWN Ð¤ÐžÐ ÐœÐÐ¢Ð˜Ð ÐžÐ’ÐÐÐ˜Ð•.
            Ð’Ð«Ð’Ð•Ð”Ð˜ Ð¢ÐžÐ›Ð¬ÐšÐž Ð§Ð˜Ð¡Ð¢Ð«Ð™ JSON           
            """

    async def execute(self, query_request: QueryRequest) -> LLMResponse:
        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð¸Ð»Ð¸ ÑÐ¾Ð·Ð´Ð°ÐµÐ¼ chat_id
        chat_id = await self._get_or_create_chat_id(query_request)

        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ‚ÐµÐºÑƒÑ‰ÐµÐµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ñ‡Ð°Ñ‚Ð°
        chat = await self.chat_storage.get_chat(chat_id)
        current_question_count = chat['question_count'] if chat else 0

        print(
            f"ðŸ” DEBUG: ÐÐ°Ñ‡Ð°Ð»Ð¾ execute. Ð’Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð²: {current_question_count}, ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ: {query_request.user_input[:50]}...")

        # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
        await self.chat_storage.add_message(chat_id, "user", query_request.user_input)

        # ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾
        await self.chat_storage.optimize_history(chat_id, query_request.max_history_messages)

        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¿Ð¾Ð»Ð½ÑƒÑŽ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
        full_messages = await self.chat_storage.get_chat_messages_with_timestamp(chat_id)

        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼, Ð½ÑƒÐ¶Ð½Ð¾ Ð»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð°Ð½Ð°Ð»Ð¸Ð·
        should_use_analysis = (
                current_question_count == 7
        )

        print(f"ðŸ” DEBUG: should_use_analysis = {should_use_analysis}")

        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð´Ð»Ñ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°
        if should_use_analysis:
            print("ðŸ” DEBUG: Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°")
            messages = await self._prepare_messages_for_analysis(chat_id)
        else:
            print("ðŸ” DEBUG: Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚")
            messages = await self.chat_storage.get_chat_messages(chat_id)

        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¾Ñ‚ LLM
        assistant_response = await self.llm_provider.generate_response(messages)

        print(f"ðŸ” DEBUG: ÐžÑ‚Ð²ÐµÑ‚ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð°: {assistant_response[:100]}...")

        # ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ ÐµÑÐ»Ð¸ ÑÑ‚Ð¾ Ð°Ð½Ð°Ð»Ð¸Ð·
        final_response = assistant_response
        is_analysis = False

        if should_use_analysis:
            parsed_result = self.analysis_service.parse_llm_response(assistant_response)
            if parsed_result:
                final_response = parsed_result
                is_analysis = True
                print("âœ… Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ñ€Ð°ÑÐ¿Ð°Ñ€ÑÐµÐ½")
            else:
                print("âŒ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ñ€Ð°ÑÐ¿Ð°Ñ€ÑÐ¸Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÑÑ‹Ñ€Ð¾Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚")

        # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¾Ñ‚Ð²ÐµÑ‚ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð° Ð¸ ÑƒÐ²ÐµÐ»Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ ÑÑ‡ÐµÑ‚Ñ‡Ð¸Ðº
        await self.chat_storage.add_message(chat_id, "assistant",
                                            final_response.to_json() if is_analysis else assistant_response)
        await self.chat_storage.increment_question_count(chat_id)

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÑÑ‚Ð°Ñ‚ÑƒÑ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ
        is_completed = await self.chat_storage.is_chat_completed(chat_id)

        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¹ ÑÑ‡ÐµÑ‚Ñ‡Ð¸Ðº Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð²
        updated_chat = await self.chat_storage.get_chat(chat_id)
        question_count = updated_chat['question_count'] if updated_chat else 0

        print(f"ðŸ” DEBUG: ÐšÐ¾Ð½ÐµÑ† execute. Ð’Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð² ÑÑ‚Ð°Ð»Ð¾: {question_count}, Ð—Ð°Ð²ÐµÑ€ÑˆÐµÐ½: {is_completed}")

        return LLMResponse(
            content=final_response,
            chat_id=chat_id,
            is_completed=is_completed,
            question_count=question_count,
            total_questions=query_request.max_questions,
            is_analysis=is_analysis
        )

    async def execute_stream(self, query_request: QueryRequest) -> AsyncGenerator[LLMStreamResponse, None]:
        """Streaming Ð²ÐµÑ€ÑÐ¸Ñ execute"""
        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð¸Ð»Ð¸ ÑÐ¾Ð·Ð´Ð°ÐµÐ¼ chat_id
        chat_id = await self._get_or_create_chat_id(query_request)

        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ‚ÐµÐºÑƒÑ‰ÐµÐµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ñ‡Ð°Ñ‚Ð°
        chat = await self.chat_storage.get_chat(chat_id)
        current_question_count = chat['question_count'] if chat else 0

        print(
            f"ðŸ” DEBUG: ÐÐ°Ñ‡Ð°Ð»Ð¾ execute_stream. Ð’Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð²: {current_question_count}, ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ: {query_request.user_input[:50]}...")

        # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
        await self.chat_storage.add_message(chat_id, "user", query_request.user_input)

        # ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾
        await self.chat_storage.optimize_history(chat_id, query_request.max_history_messages)

        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¿Ð¾Ð»Ð½ÑƒÑŽ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
        full_messages = await self.chat_storage.get_chat_messages_with_timestamp(chat_id)

        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼, Ð½ÑƒÐ¶Ð½Ð¾ Ð»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð°Ð½Ð°Ð»Ð¸Ð·
        should_use_analysis = (
                current_question_count == 7 and  # Ð£Ð¶Ðµ Ð·Ð°Ð´Ð°Ð½Ð¾ 7 Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð²
                len(full_messages) > 1 and
                full_messages[-1]["role"] == "user"  # ÐŸÐ¾ÑÐ»ÐµÐ´Ð½ÐµÐµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ - Ð¾Ñ‚Ð²ÐµÑ‚ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
        )

        print(f"ðŸ” DEBUG: should_use_analysis = {should_use_analysis}")

        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð´Ð»Ñ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°
        if should_use_analysis:
            print("ðŸ” DEBUG: Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°")
            messages = await self._prepare_messages_for_analysis(chat_id)
        else:
            print("ðŸ” DEBUG: Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚")
            messages = await self.chat_storage.get_chat_messages(chat_id)

        # Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¿Ð¾ Ñ‡Ð°ÑÑ‚ÑÐ¼
        full_response = ""

        # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ streaming Ð¾Ñ‚Ð²ÐµÑ‚
        async for chunk in self.llm_provider.generate_response_stream(messages):
            full_response += chunk

            # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ ÐºÐ°Ð¶Ð´ÑƒÑŽ Ñ‡Ð°ÑÑ‚ÑŒ
            yield LLMStreamResponse(
                content_chunk=chunk,
                chat_id=chat_id,
                is_completed=False,
                question_count=current_question_count,
                total_questions=query_request.max_questions,
                is_final_chunk=False,
                is_analysis=should_use_analysis
            )

        # ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ ÐµÑÐ»Ð¸ ÑÑ‚Ð¾ Ð°Ð½Ð°Ð»Ð¸Ð·
        final_response = full_response
        is_analysis = should_use_analysis

        if should_use_analysis:
            parsed_result = self.analysis_service.parse_llm_response(full_response)
            if parsed_result:
                final_response = parsed_result.to_json()
                print("âœ… Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ñ€Ð°ÑÐ¿Ð°Ñ€ÑÐµÐ½")
            else:
                print("âŒ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ñ€Ð°ÑÐ¿Ð°Ñ€ÑÐ¸Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÑÑ‹Ñ€Ð¾Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚")

        # ÐŸÐ¾ÑÐ»Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ streaming - ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚
        await self.chat_storage.add_message(chat_id, "assistant", final_response)

        # Ð£Ð²ÐµÐ»Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ ÑÑ‡ÐµÑ‚Ñ‡Ð¸Ðº Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð²
        await self.chat_storage.increment_question_count(chat_id)

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÑÑ‚Ð°Ñ‚ÑƒÑ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ
        is_completed = await self.chat_storage.is_chat_completed(chat_id)

        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð½Ñ‹Ð¹ ÑÑ‡ÐµÑ‚Ñ‡Ð¸Ðº
        updated_chat = await self.chat_storage.get_chat(chat_id)
        question_count = updated_chat['question_count'] if updated_chat else 0

        print(f"ðŸ” DEBUG: ÐšÐ¾Ð½ÐµÑ† execute_stream. Ð’Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð² ÑÑ‚Ð°Ð»Ð¾: {question_count}, Ð—Ð°Ð²ÐµÑ€ÑˆÐµÐ½: {is_completed}")

        # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ chunk
        yield LLMStreamResponse(
            content_chunk="",
            chat_id=chat_id,
            is_completed=is_completed,
            question_count=question_count,
            total_questions=query_request.max_questions,
            is_final_chunk=True,
            is_analysis=is_analysis
        )

    async def _prepare_messages_for_analysis(self, chat_id: str) -> list:
        """ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÑ‚ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²"""
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð¸Ð°Ð»Ð¾Ð³ (Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹) Ð±ÐµÐ· ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°
        all_messages = await self.chat_storage.get_chat_messages_with_timestamp(chat_id)

        dialog_messages = []
        for msg in all_messages:
            if msg["role"] in ["user", "assistant"]:
                dialog_messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })

        # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð² Ð½Ð°Ñ‡Ð°Ð»Ð¾
        analysis_messages = [
            {"role": "system", "content": self.analysis_prompt},
            *dialog_messages
        ]

        print(f"ðŸ” DEBUG: Ð”Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð»ÐµÐ½Ð¾ {len(analysis_messages)} ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹")
        return analysis_messages

    async def _get_or_create_chat_id(self, request: QueryRequest) -> str:
        """Ð¡Ð¾Ð·Ð´Ð°ÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ñ‡Ð°Ñ‚ Ð¸Ð»Ð¸ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ð¹"""
        # Ð•ÑÐ»Ð¸ Ð¿ÐµÑ€ÐµÐ´Ð°Ð½ chat_id Ð¸ Ñ‡Ð°Ñ‚ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚ - Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÐµÐ³Ð¾
        if request.chat_id:
            existing_chat = await self.chat_storage.get_chat(request.chat_id)
            if existing_chat:
                return request.chat_id

        # Ð˜Ð½Ð°Ñ‡Ðµ ÑÐ¾Ð·Ð´Ð°ÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ð¹ Ñ‡Ð°Ñ‚ Ñ ÑƒÑ‡ÐµÑ‚Ð¾Ð¼ list_user_psych_status
        return await self.chat_storage.create_chat(
            list_user_psych_status=request.list_user_psych_status,
            max_questions=request.max_questions
        )


class UseCaseFactory:
    @staticmethod
    async def create_burnout_survey_use_case(mongo_connection_string: str) -> QueryLLMUseCase:
        llm_provider = DeepSeekLLM()
        chat_storage = MongoDBChatStorage(mongo_connection_string)

        return QueryLLMUseCase(llm_provider, chat_storage)